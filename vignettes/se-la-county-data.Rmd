---
title: "Reproducing LA County Standard Errors from [@buja2019modelsasapproximationspart1]"
author: "Riccardo Fogliato, Shamindra Shrotriya"
output: rmarkdown::html_vignette
package: maars    
bibliography: ../inst/REFERENCES.bib
resource_files:
  - figures/buja1_table1.png
vignette: >
  %\VignetteIndexEntry{Reproducing LA County Standard Errors from [@buja2019modelsasapproximationspart1]}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

``` {r  include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  linewidth = 80,
  fig.align = 'center',
  warning = FALSE,
  message = FALSE
)
```

```{r create_chunk_options, include=FALSE}
source(here::here("R", "scripts_and_filters", "create-chunk-options.R"))
source(here::here("R", "scripts_and_filters", "wrap-lines.R"))
```

## Setup and Installation

First, we'll load the `maars` package, and other `tidyverse` analysis packages.
If you are a new user, you will need to manually run the following commands in a
new in an `R` session before running the vignette code below:

``` {r install_maar, eval=FALSE}
install.packages(pkgs = c("tidyverse", "vroom", "glue", "magrittr", 
                          "knitr", "kableExtra", "remotes"))
remotes::install_github('shamindras/maars', force = TRUE)
```
```{r load_all, include = FALSE}
devtools::load_all()
```


We will explicitly load the `tidyverse` set of packages to get access to the
`magrittr` pipe (`%>%`) operator. We will also set a global random seed for
reproducibility purposes.

``` {r load_pkgs}
library(magrittr)
set.seed(25422267)
```

For all other functions we will refer to them explicitly using the
`package::function` reference method to avoid ambiguity in the source package of
the specific function used (to avoid function name conflicts).

## Goal of Analysis

We try to replicate the results of Table 1 in
[@buja2019modelsasapproximationspart1], as shown below:

``` {r table1_img, echo=FALSE, fig.cap="Table 1 from [@buja2019modelsasapproximationspart1]", out.width = '90%'}
# knitr::include_graphics(here::here("vignettes", "figures", "buja1_table1.png"))
knitr::include_graphics(path = "figures/buja1_table1.png")
```

## Loading the data

Let's load the LA County Homeless persons data as used in
[@buja2019modelsasapproximationspart1] and briefly examine it. 
The dataset is already included in the package. 

``` {r load_data}
# LA County source data url, use glue to split string in easy to read format
# let's load 
data('la_county', package = 'maars')
la_county %>% dim(x = .)
```

Let's view the first few rows to understand the structure of the data frame.

``` {r load_data_rows}
la_county %>% 
  head(x = .) %>% 
  knitr::kable(x = ., format = "html", digits = 2, align = "c") %>% 
  kableExtra::kable_styling(position = "center")
```

It is already in tidy format [@wickham2014tidydata]. So we have imported the 
data correctly, given that we match the column names in Table 1 of the [@buja2019modelsasapproximationspart1] paper.

## Fitting the OLS model

We now fit an linear model of the count of homeless people (`StreetTotal`) as
the response variable, against the other covariates using Ordinary Least Squares
(OLS).

``` {r mod_fit_lm}
mod_fit <- stats::lm(formula = StreetTotal ~ ., data = la_county)
```


### Estimating the variance

Let's estimate the variance of the regression coefficients for the fitted model via 

* sandwich estimator (see
[@white1980heteroskedasticconsistentcovest,
@white1980usinglsapproxunknownregfuncs]). 
* (n-out-of-n) empirical bootstrap with $B=10^{3}$ replications.
* multiplier bootstrap with $B=10^{3}$ replications with Rademacher weights.
* residual bootstrap with $B=10^{3}$ replications.

To do so, we will use `comp_var`. 
The sandwich estimator of the variance is always computed by default.

```{r comp_var}
mms_fit <- comp_var(mod_fit = mod_fit,
                    boot_emp = list(B=10^3),
                    boot_mul = list(B=10^3, weights_type='rademacher'),
                    boot_res = list(B=10^3))
```

We have obtained an object of class `r class(mms_fit)`
We also have all the ingredients necessary to reproduce table 1 of the 
[@buja2019modelsasapproximationspart1] paper. 
Technically, we will only need the empirical bootstrap and sandiwch estimates.
In `maars`, the estimates, standard errors, t-statistics, and p-values can be 
obtained in a `tidy` output via `get_summary`. 
This function returns the sandwich, again, returned by default, unless specified otherwise. 
Let's first extract the summary of `mms_fit`.

```{r mms_summary}
mms_summary <- get_summary(mms_fit, boot_emp = TRUE, well_specified = TRUE)

head(mms_summary) %>%
  knitr::kable(x = ., format = "html", digits = 2, align = "c") %>% 
  kableExtra::kable_styling(position = "center")
```
As we can see, the output is in tidy format as a `tibble` object. This makes it
much more readily amenable for additional transformations using the `tidyverse`
set of packages.

We can now reformat the output of `get_summary` to make it more easily 
comparable to table 1 in the paper.
This can be achieved by first dropping the p-values (which are not needed),
followed by the computation of the ratios of the variances, and last by a `pivot_wider`.

```{r reproduce_table1}
mms_summary %>%
  # drop the p-values
  dplyr::filter(stat_type != 'p.value') %>%
  # compute the variances
  tidyr::pivot_wider(names_from = c(stat_type, var_type_abb), 
                     values_from = stat_val) %>%
  # compute the ratios
  dplyr::mutate(ratio_emp_vs_lm = std.error_emp/std.error_lm,
                ratio_sand_vs_lm = std.error_sand/std.error_emp,
                ratio_sand_vs_emp = std.error_sand/std.error_emp) %>%
  # reorder the variables
  dplyr::select(term, 
                estimate, 
                starts_with("std.error"), 
                starts_with("ratio"), 
                starts_with("statistic")) %>%
  # rename the variables
  purrr::set_names(x = ., nm = c("Term", "$\\widehat{\\beta}_{j}$", 
                                 "$SE_{\\text{lin}}$", 
                                 "$SE_{\\text{boot}}$", 
                                 "$SE_{\\text{sand}}$",
                                 "$\\frac{SE_{\\text{boot}}}{SE_{\\text{lin}}}$",
                                 "$\\frac{SE_{\\text{sand}}}{SE_{\\text{lin}}}$", 
                                 "$\\frac{SE_{\\text{sand}}}{SE_{\\text{boot}}}$",
                                 "$t_{\\text{lin}}$", 
                                 "$t_{\\text{boot}}$",
                                 "$t_{\\text{sand}}$")) %>%
  knitr::kable(
    x = ., format = "html", digits = 2,
    align = "c", escape = TRUE
  ) %>%
  kableExtra::kable_styling(position = "center")
```


We can obtain the other estimates of the variance (i.e., from 
residual and multiplier bootstraps) that we computed in `comp_var` 
in tidy format by specifying these additional arguments within `get_summary`.

```{r get_summary_all_estimates}
mms_summary <- get_summary(mms_fit, 
                           boot_emp = TRUE,
                           boot_res = TRUE,
                           boot_mul = TRUE,
                           well_specified = TRUE)
head(mms_summary) %>%
  knitr::kable(x = ., format = "html", digits = 2, align = "c") %>% 
  kableExtra::kable_styling(position = "center")
```



### Visualizing the estimates

We may want to compare some of the estimates that we obtained through `comp_var` 
in a plot. 
Confidence intervals and normality of the (bootstrap) estimates 
reprent two (arguably) interesting statistics the data scientist/researcher may
want to analyze. The corresponding plots are returned by default when `plot` is
called on a `maars` object, together with six other "typical" `lm` plots. 
The plots can be visualized sequentially calling `plot` on an object of class 
`r class(mms_fit)` (e.g., `plot(mms_fit, which=c(1,2,7))`. 
Alternatively, we can store them in a list via `get_ols_diag_plots`, 
as we do below.

```{r obtain_plots}
mms_plots <- get_ols_diag_plots(mms_fit)
```

Let's now visually compare the various types of (95%) confidence intervals.
```{r compare_confint, fig.dim = c(6,5)}
purrr::pluck(mms_plots, 'p7')
```
Let's now visualize the distribution of the bootstrap estimate to check whether it well 
approximates the normal distribution.
```{r visualize_boot_estimates, fig.dim = c(6,5)}
purrr::pluck(mms_plots, 'p8')
```



## References
